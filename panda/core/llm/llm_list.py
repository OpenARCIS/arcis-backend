MISTRAL_AI = {
    "codestral-2405": {
        "parameters": "22B",
        "context": "32k",
        "usage": "Code completion & generation",
        "speed": "Fast"
    },
    "codestral-2501": {
        "parameters": "<100B",
        "context": "256k",
        "usage": "Advanced code generation (v2)",
        "speed": "Fast"
    },
    "codestral-mamba-2407": {
        "parameters": "Unknown",
        "context": "256k",
        "usage": "Long-context code generation",
        "speed": "Fast"
    },
    "ministral-3b-2410": {
        "parameters": "3B",
        "context": "128k",
        "usage": "Edge AI & low-latency tasks",
        "speed": "Ultra-fast"
    },
    "ministral-8b-2410": {
        "parameters": "8B",
        "context": "128k",
        "usage": "Complex edge tasks",
        "speed": "Ultra-fast"
    },
    "mistral-embed": {
        "parameters": "N/A",
        "context": "N/A",
        "usage": "Text embeddings",
        "speed": "Fast"
    },
    "mistral-large-2402": {
        "parameters": "123B",
        "context": "32k",
        "usage": "Complex reasoning (Legacy)",
        "speed": "Moderate"
    },
    "mistral-large-2407": {
        "parameters": "123B",
        "context": "128k",
        "usage": "Flagship reasoning (v2)",
        "speed": "Moderate"
    },
    "mistral-large-2411": {
        "parameters": "123B",
        "context": "128k",
        "usage": "Latest flagship reasoning",
        "speed": "Moderate"
    },
    "mistral-medium": {
        "parameters": "Unknown",
        "context": "32k",
        "usage": "Balanced performance (Legacy)",
        "speed": "Moderate"
    },
    "mistral-moderation-2411": {
        "parameters": "Unknown",
        "context": "N/A",
        "usage": "Content safety & moderation",
        "speed": "Fast"
    },
    "mistral-saba-2502": {
        "parameters": "Unknown",
        "context": "N/A",
        "usage": "Middle East & South Asia optimized",
        "speed": "Fast"
    },
    "mistral-small-2402": {
        "parameters": "Small",
        "context": "32k",
        "usage": "Cost-effective reasoning",
        "speed": "Fast"
    },
    "mistral-small-2409": {
        "parameters": "22B",
        "context": "32k",
        "usage": "Improved cost-effective reasoning",
        "speed": "Fast"
    },
    "mistral-small-2501": {
        "parameters": "24B",
        "context": "32k",
        "usage": "Latest small model",
        "speed": "Fast"
    },
    "mistral-small-2503": {
        "parameters": "24B",
        "context": "32k",
        "usage": "Updated small model",
        "speed": "Fast"
    },
    "open-mistral-7b": {
        "parameters": "7B",
        "context": "32k",
        "usage": "General purpose & efficiency",
        "speed": "Fast"
    },
    "open-mistral-nemo": {
        "parameters": "12B",
        "context": "128k",
        "usage": "Research & experimentation",
        "speed": "Fast"
    },
    "open-mixtral-8x22b": {
        "parameters": "141B",
        "context": "64k",
        "usage": "Open-source flagship prowess",
        "speed": "Moderate"
    },
    "open-mixtral-8x7b": {
        "parameters": "47B",
        "context": "32k",
        "usage": "High throughput text generation",
        "speed": "High"
    },
    "pixtral-12b-2409": {
        "parameters": "12B",
        "context": "128k",
        "usage": "Multimodal (Text + Images)",
        "speed": "Fast"
    },
    "pixtral-large-2411": {
        "parameters": "124B",
        "context": "128k",
        "usage": "Flagship multimodal",
        "speed": "Moderate"
    }
}

CEREBRAS = {
    "gpt-oss-120b": {
        "parameters": "120B",
        "context": "128k",
        "usage": "Open-source heavy reasoning",
        "speed": "~3000 tps"
    },
    "llama-3.3-70b": {
        "parameters": "70B",
        "context": "128k",
        "usage": "High-performance general reasoning",
        "speed": "~2100 tps"
    },
    "llama3.1-8b": {
        "parameters": "8B",
        "context": "128k",
        "usage": "Fast, lightweight reasoning",
        "speed": "Ultra-fast"
    },
    "qwen-3-235b-a22b-instruct-2507": {
        "parameters": "235B",
        "context": "131k",
        "usage": "Massive scale reasoning",
        "speed": "~1500 tps"
    },
    "qwen-3-32b": {
        "parameters": "32B",
        "context": "131k",
        "usage": "Balanced performance & speed",
        "speed": "~2600 tps"
    },
    "zai-glm-4.6": {
        "parameters": "Unknown",
        "context": "Unknown",
        "usage": "Unknown",
        "speed": "Unknown"
    },
    "zai-glm-4.7": {
        "parameters": "Unknown",
        "context": "Unknown",
        "usage": "Unknown",
        "speed": "Unknown"
    }
}

GROQ = {
    "allam-2-7b": {
        "parameters": "7B",
        "context": "Unknown",
        "usage": "Specialized Arabic/English",
        "speed": "Fast"
    },
    "canopylabs/orpheus-arabic-saudi": {
        "parameters": "Unknown",
        "context": "Unknown",
        "usage": "Saudi Arabic dialect",
        "speed": "Fast"
    },
    "canopylabs/orpheus-v1-english": {
        "parameters": "Unknown",
        "context": "Unknown",
        "usage": "English language tasks",
        "speed": "Fast"
    },
    "groq/compound": {
        "parameters": "Unknown",
        "context": "Unknown",
        "usage": "Groq proprietary compound",
        "speed": "Ultra-fast"
    },
    "groq/compound-mini": {
        "parameters": "Unknown",
        "context": "Unknown",
        "usage": "Small Groq compound",
        "speed": "Ultra-fast"
    },
    "llama-3.1-8b-instant": {
        "parameters": "8B",
        "context": "131k",
        "usage": "Instant text generation",
        "speed": "~560 tps"
    },
    "llama-3.3-70b-versatile": {
        "parameters": "70B",
        "context": "128k",
        "usage": "Versatile general intelligence",
        "speed": "~276 tps"
    },
    "meta-llama/llama-4-maverick-17b-128e-instruct": {
        "parameters": "17B",
        "context": "Unknown",
        "usage": "Maverick variant",
        "speed": "Fast"
    },
    "meta-llama/llama-4-scout-17b-16e-instruct": {
        "parameters": "17B",
        "context": "Unknown",
        "usage": "Scout variant",
        "speed": "Fast"
    },
    "meta-llama/llama-guard-4-12b": {
        "parameters": "12B",
        "context": "8k",
        "usage": "Safety & moderation",
        "speed": "Fast"
    },
    "meta-llama/llama-prompt-guard-2-22m": {
        "parameters": "22M",
        "context": "Unknown",
        "usage": "Lightweight prompt safety",
        "speed": "Ultra-fast"
    },
    "meta-llama/llama-prompt-guard-2-86m": {
        "parameters": "86M",
        "context": "Unknown",
        "usage": "Lightweight prompt safety",
        "speed": "Ultra-fast"
    },
    "moonshotai/kimi-k2-instruct": {
        "parameters": "Unknown",
        "context": "128k",
        "usage": "Reasoning & instruction following",
        "speed": "Fast"
    },
    "moonshotai/kimi-k2-instruct-0905": {
        "parameters": "Unknown",
        "context": "128k",
        "usage": "Updated Kimi model",
        "speed": "Fast"
    },
    "openai/gpt-oss-120b": {
        "parameters": "120B",
        "context": "128k",
        "usage": "Open-source GPT alternative",
        "speed": "~300 tps"
    },
    "openai/gpt-oss-20b": {
        "parameters": "20B",
        "context": "128k",
        "usage": "Smaller GPT alternative",
        "speed": "Fast"
    },
    "openai/gpt-oss-safeguard-20b": {
        "parameters": "20B",
        "context": "Unknown",
        "usage": "Safeguard model",
        "speed": "Fast"
    },
    "qwen/qwen3-32b": {
        "parameters": "32B",
        "context": "131k",
        "usage": "Qwen coding & general tasks",
        "speed": "Fast"
    },
    "whisper-large-v3": {
        "parameters": "1.5B",
        "context": "N/A",
        "usage": "Speech recognition",
        "speed": "Fast"
    },
    "whisper-large-v3-turbo": {
        "parameters": "1.5B",
        "context": "N/A",
        "usage": "Fast speech recognition",
        "speed": "Ultra-fast"
    }
}

OPENROUTER = {
    "allenai/molmo-2-8b:free": {
        "parameters": "8B",
        "context": "128k",
        "usage": "Video grounding & counting",
        "speed": "40 - 60 tps"
    },
    "xiaomi/mimo-v2-flash:free": {
        "parameters": "309B (15B active)",
        "context": "256k",
        "usage": "Production-grade agents; high-speed coding & math reasoning",
        "speed": "150+ tps"
    },
    "nvidia/nemotron-3-nano-30b-a3b:free": {
        "parameters": "32B (3.5B active)",
        "context": "1M",
        "usage": "Math reasoning & tool-use",
        "speed": "80 - 120 tps"
    },
    "mistralai/devstral-2512:free": {
        "parameters": "123B",
        "context": "256k",
        "usage": "Agentic software engineering",
        "speed": "15-25 tps"
    },
    "arcee-ai/trinity-mini:free": {
        "parameters": "26B (3B active)",
        "context": "128k",
        "usage": "Multi-step agent workflows",
        "speed": "100 - 150 tps"
    },
    "tngtech/tng-r1t-chimera:free": {
        "parameters": "671B (37B active)",
        "context": "131k",
        "usage": "Balanced reasoning; reduces verbose output by 40% vs R1",
        "speed": "15 - 20 tps"
    },
    "nvidia/nemotron-nano-12b-v2-vl:free": {
        "parameters": "12B",
        "context": "128k",
        "usage": "On-device vision-language",
        "speed": "45 - 70 tps"
    },
    "nvidia/nemotron-nano-9b-v2:free": {
        "parameters": "9B",
        "context": "128k",
        "usage": "Low-latency agentic tasks",
        "speed": "90 - 130 tps"
    },
    "openai/gpt-oss-120b:free": {
        "parameters": "120B",
        "context": "128k",
        "usage": "Open-source heavy reasoning",
        "speed": "10 - 18 tps"
    },
    "openai/gpt-oss-20b:free": {
        "parameters": "20B",
        "context": "128k",
        "usage": "Consumer-grade reasoning",
        "speed": "50 - 80 tps"
    },
    "z-ai/glm-4.5-air:free": {
        "parameters": "100B+ (MoE)",
        "context": "128k",
        "usage": "Balanced performance/speed",
        "speed": "30 - 50 tps"
    },
    "qwen/qwen3-coder:free": {
        "parameters": "30B - 480B",
        "context": "256k+",
        "usage": "Expert coding & repo analysis",
        "speed": "12 - 45 tps"
    },
    "moonshotai/kimi-k2:free": {
        "parameters": "1T (32B active)",
        "context": "128k",
        "usage": "Sequential tool calls/Thinking",
        "speed": "15 - 20 tps"
    },
    "cognitivecomputations/dolphin-mistral-24b-venice-edition:free": {
        "parameters": "24B",
        "context": "32k",
        "usage": "Uncensored creative writing",
        "speed": "40 - 65 tps"
    },
    "google/gemma-3n-e2b-it:free": {
        "parameters": "5B / 8B",
        "context": "32k",
        "usage": "Real-time on-device audio/video",
        "speed": "N/A"
    },
    "tngtech/deepseek-r1t2-chimera:free": {
        "parameters": "671B (37B active)",
        "context": "163k",
        "usage": "Advanced STEM reasoning; 2x faster than R1-0528",
        "speed": "20 - 30 tps"
    },
    "deepseek/deepseek-r1-0528:free": {
        "parameters": "671B (37B active)",
        "context": "128k",
        "usage": "SOTA mathematical reasoning",
        "speed": "10 - 25 tps"
    },
    "google/gemma-3n-e4b-it:free": {
        "parameters": "8B (3B memory)",
        "context": "32k",
        "usage": "Mobile Multimodal; real-time video/audio on edge",
        "speed": "80 - 120 tps"
    },
    "qwen/qwen3-4b:free": {
        "parameters": "4B",
        "context": "262k",
        "usage": "Local long-context; handling massive codebases on laptops",
        "speed": "120 - 160 tps"
    },
    "mistralai/mistral-small-3.1-24b-instruct:free": {
        "parameters": "24B",
        "context": "128k",
        "usage": "Enterprise-grade dialogue",
        "speed": "150+ tps"
    },
    "google/gemma-3-4b-it:free": {
        "parameters": "4B",
        "context": "128k",
        "usage": "Fast mobile vision tasks",
        "speed": "120+ tps"
    },
    "google/gemma-3-12b-it:free": {
        "parameters": "12B",
        "context": "128k",
        "usage": "Mid-range multimodal chat",
        "speed": "60 - 85 tps"
    },
    "google/gemma-3-27b-it:free": {
        "parameters": "27B",
        "context": "128k",
        "usage": "High-tier general multimodal",
        "speed": "30 - 45 tps"
    },
    "meta-llama/llama-3.3-70b-instruct:free": {
        "parameters": "70B",
        "context": "128k",
        "usage": "Reliable multilingual reasoning",
        "speed": "50 - 70 tps"
    },
    "meta-llama/llama-3.2-3b-instruct:free": {
        "parameters": "3B",
        "context": "128k",
        "usage": "Mobile summarization",
        "speed": "150+ tps"
    },
    "qwen/qwen-2.5-vl-7b-instruct:free": {
        "parameters": "7B",
        "context": "32k",
        "usage": "Visual grounding & OCR",
        "speed": "60 - 90 tps"
    },
    "nousresearch/hermes-3-llama-3.1-405b:free": {
        "parameters": "405B",
        "context": "128k",
        "usage": "Frontier-level open reasoning",
        "speed": "5 - 12 tps"
    },
    "meta-llama/llama-3.1-405b-instruct:free": {
        "parameters": "405B (Dense)",
        "context": "128k",
        "usage": "Enterprise foundation; synthetic data & complex translations",
        "speed": "5 - 12 tps"
    },
    "mistralai/mistral-7b-instruct:free": {
        "parameters": "7B",
        "context": "32k",
        "usage": "Lightweight general purpose",
        "speed": "90 - 110 tps"
    }
}

OPENAI = {
    "gpt-4o": {
        "parameters": "Unknown",
        "context": "128k",
        "usage": "Flagship multimodal",
        "speed": "Fast"
    },
    "gpt-4-turbo": {
        "parameters": "Unknown",
        "context": "128k",
        "usage": "High-capability reasoning",
        "speed": "Moderate"
    },
    "gpt-3.5-turbo": {
        "parameters": "Unknown",
        "context": "16k",
        "usage": "Fast, cost-effective chat",
        "speed": "Fast"
    }
}

GEMINI = {
    "gemini-1.5-flash": {
        "parameters": "Unknown",
        "context": "1M",
        "usage": "High-volume, low-latency multimodal",
        "speed": "High"
    },
    "gemini-1.5-pro": {
        "parameters": "Unknown",
        "context": "2M",
        "usage": "Complex multimodal reasoning",
        "speed": "Moderate"
    },
    "gemini-1.0-pro": {
        "parameters": "Unknown",
        "context": "32k",
        "usage": "General text & code",
        "speed": "Fast"
    }
}

ANTHROPIC = {
    "claude-3-5-sonnet-20240620": {
        "parameters": "Unknown",
        "context": "200k",
        "usage": "Highest intelligence & speed",
        "speed": "Fast"
    },
    "claude-3-opus-20240229": {
        "parameters": "Unknown",
        "context": "200k",
        "usage": "Complex task reasoning",
        "speed": "Moderate"
    },
    "claude-3-sonnet-20240229": {
        "parameters": "Unknown",
        "context": "200k",
        "usage": "Balanced enterprise workload",
        "speed": "Fast"
    },
    "claude-3-haiku-20240307": {
        "parameters": "Unknown",
        "context": "200k",
        "usage": "Instant response & speed",
        "speed": "Ultra-fast"
    }
}

OLLAMA = {}
